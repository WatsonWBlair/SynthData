{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Synthetic Data Generator\n",
    "## For Educational Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our generator\n",
    "from enhanced_data_generator import DarkPoolDataGenerator\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Dataset with Configurable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters - MODIFY THESE AS NEEDED\n",
    "NUM_TRANSACTIONS = 100000  # Dataset size\n",
    "FRAUD_RATE = 0.05          # 5% fraud rate\n",
    "RANDOM_SEED = 42           # For reproducibility\n",
    "DIFFICULTY = 'intermediate' # 'beginner', 'intermediate', 'advanced'\n",
    "\n",
    "# Initialize generator\n",
    "generator = DarkPoolDataGenerator(random_seed=RANDOM_SEED)\n",
    "\n",
    "# Generate dataset with fraud patterns\n",
    "print(f\"Generating {NUM_TRANSACTIONS:,} transactions with {FRAUD_RATE*100:.1f}% fraud rate...\")\n",
    "enhanced_df, fraud_metadata = generator.generate_enhanced_dataset(\n",
    "    num_transactions=NUM_TRANSACTIONS, \n",
    "    fraud_rate=FRAUD_RATE\n",
    ")\n",
    "\n",
    "# Create data directory if needed\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "\n",
    "# Save files\n",
    "enhanced_df.to_csv('./data/enhanced_raw_data.csv', index=False)\n",
    "pd.DataFrame(fraud_metadata).to_csv('./data/fraud_patterns_metadata.csv', index=False)\n",
    "\n",
    "print(f\"\\nDataset Shape: {enhanced_df.shape}\")\n",
    "print(f\"Fraud Patterns Injected: {len(fraud_metadata):,}\")\n",
    "print(f\"\\nFiles saved to ./data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Student Dataset (No Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate multiple unique datasets for different student teams\nNUM_STUDENT_DATASETS = 5  # Number of teams\n\n# Generate unique datasets\nprint(f\"Generating {NUM_STUDENT_DATASETS} unique student datasets...\")\ngenerated_files = generator.generate_student_datasets(\n    num_datasets=NUM_STUDENT_DATASETS,\n    num_transactions=50000,\n    fraud_rate=FRAUD_RATE,\n    base_seed=RANDOM_SEED\n)\n\nprint(f\"\\nâœ… Generated {NUM_STUDENT_DATASETS} unique datasets\")\nprint(f\"ðŸ“ Files saved in: ./data/student_datasets/\")\nprint(f\"ðŸ”‘ Keep answer keys secure!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Overview and Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information\n",
    "print(\"Dataset Overview:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Transactions: {len(enhanced_df):,}\")\n",
    "print(f\"Date Range: {enhanced_df['Timestamp'].min()} to {enhanced_df['Timestamp'].max()}\")\n",
    "print(f\"\\nAmount Statistics:\")\n",
    "print(f\"  Min: ${enhanced_df['Amount'].min():,.2f}\")\n",
    "print(f\"  Max: ${enhanced_df['Amount'].max():,.2f}\")\n",
    "print(f\"  Mean: ${enhanced_df['Amount'].mean():,.2f}\")\n",
    "print(f\"  Median: ${enhanced_df['Amount'].median():,.2f}\")\n",
    "print(f\"  Std Dev: ${enhanced_df['Amount'].std():,.2f}\")\n",
    "print(f\"\\nUnique Values:\")\n",
    "print(f\"  Merchants: {enhanced_df['Merchant'].nunique()}\")\n",
    "print(f\"  Accounts: {enhanced_df['AccountID'].nunique()}\")\n",
    "print(f\"  Locations: {enhanced_df['Location'].nunique()}\")\n",
    "print(f\"  Transaction Types: {enhanced_df['TransactionType'].nunique()}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample Transactions:\")\n",
    "enhanced_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Benford's Law Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benford_analysis(data, amount_column='Amount'):\n",
    "    \"\"\"Analyze Benford's Law compliance\"\"\"\n",
    "    amounts = data[amount_column].dropna()\n",
    "    first_digits = amounts.astype(str).str[0].astype(int)\n",
    "    \n",
    "    # Calculate observed frequencies\n",
    "    observed_counts = first_digits.value_counts().sort_index()\n",
    "    observed_proportions = observed_counts / len(first_digits)\n",
    "    \n",
    "    # Calculate expected Benford proportions\n",
    "    benford_proportions = pd.Series(\n",
    "        [np.log10(1 + 1/d) for d in range(1, 10)], \n",
    "        index=range(1, 10)\n",
    "    )\n",
    "    \n",
    "    # Ensure alignment\n",
    "    for digit in range(1, 10):\n",
    "        if digit not in observed_proportions.index:\n",
    "            observed_proportions[digit] = 0\n",
    "    observed_proportions = observed_proportions.sort_index()\n",
    "    \n",
    "    # Chi-squared test\n",
    "    expected_counts = benford_proportions * len(first_digits)\n",
    "    chi_squared = np.sum((observed_counts - expected_counts)**2 / expected_counts)\n",
    "    p_value = 1 - stats.chi2.cdf(chi_squared, df=8)\n",
    "    \n",
    "    return {\n",
    "        'observed': observed_proportions,\n",
    "        'expected': benford_proportions,\n",
    "        'chi_squared': chi_squared,\n",
    "        'p_value': p_value,\n",
    "        'compliant': p_value > 0.05\n",
    "    }\n",
    "\n",
    "# Perform analysis\n",
    "benford_results = benford_analysis(enhanced_df)\n",
    "\n",
    "# Visualize Benford's Law\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "digits = range(1, 10)\n",
    "width = 0.35\n",
    "\n",
    "# Distribution comparison\n",
    "ax1.bar([d - width/2 for d in digits], benford_results['expected'], \n",
    "        width, label=\"Benford's Law\", alpha=0.7, color='red')\n",
    "ax1.bar([d + width/2 for d in digits], benford_results['observed'], \n",
    "        width, label='Observed', alpha=0.7, color='blue')\n",
    "ax1.set_xlabel('First Digit')\n",
    "ax1.set_ylabel('Proportion')\n",
    "ax1.set_title('Benford\\'s Law Compliance')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Deviation plot\n",
    "deviations = benford_results['observed'] - benford_results['expected']\n",
    "colors = ['red' if d > 0 else 'blue' for d in deviations]\n",
    "ax2.bar(digits, deviations, color=colors, alpha=0.7)\n",
    "ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "ax2.set_xlabel('First Digit')\n",
    "ax2.set_ylabel('Deviation from Expected')\n",
    "ax2.set_title('Deviations from Benford\\'s Law')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nBenford's Law Analysis:\")\n",
    "print(f\"Chi-squared: {benford_results['chi_squared']:.2f}\")\n",
    "print(f\"P-value: {benford_results['p_value']:.4f}\")\n",
    "if benford_results['compliant']:\n",
    "    print(\"âœ… Dataset follows Benford's Law (natural distribution)\")\n",
    "else:\n",
    "    print(\"âš ï¸ Dataset deviates from Benford's Law\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fraud Pattern Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fraud metadata\n",
    "fraud_df = pd.DataFrame(fraud_metadata)\n",
    "\n",
    "if len(fraud_df) > 0:\n",
    "    # Mark fraudulent transactions\n",
    "    enhanced_df['is_fraud'] = enhanced_df.index.isin(fraud_df['index'])\n",
    "    \n",
    "    # Visualize fraud distribution\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Fraud type distribution\n",
    "    fraud_types = fraud_df['type'].value_counts()\n",
    "    axes[0, 0].pie(fraud_types.values, labels=fraud_types.index, autopct='%1.1f%%')\n",
    "    axes[0, 0].set_title('Fraud Pattern Types Distribution')\n",
    "    \n",
    "    # 2. Amount distribution: Normal vs Fraud\n",
    "    axes[0, 1].hist(enhanced_df[~enhanced_df['is_fraud']]['Amount'], bins=50, \n",
    "                    alpha=0.5, label='Normal', color='blue', density=True)\n",
    "    axes[0, 1].hist(enhanced_df[enhanced_df['is_fraud']]['Amount'], bins=50, \n",
    "                    alpha=0.5, label='Fraud', color='red', density=True)\n",
    "    axes[0, 1].set_xlabel('Amount ($)')\n",
    "    axes[0, 1].set_ylabel('Density')\n",
    "    axes[0, 1].set_title('Amount Distribution: Normal vs Fraudulent')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].set_xlim(0, 200000)\n",
    "    \n",
    "    # 3. Fraud by merchant\n",
    "    fraud_by_merchant = enhanced_df.groupby('Merchant')['is_fraud'].mean() * 100\n",
    "    axes[1, 0].bar(fraud_by_merchant.index, fraud_by_merchant.values)\n",
    "    axes[1, 0].set_xlabel('Merchant')\n",
    "    axes[1, 0].set_ylabel('Fraud Rate (%)')\n",
    "    axes[1, 0].set_title('Fraud Rate by Merchant')\n",
    "    axes[1, 0].axhline(y=FRAUD_RATE*100, color='red', linestyle='--', alpha=0.5, label='Expected Rate')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # 4. Temporal pattern\n",
    "    enhanced_df['Timestamp'] = pd.to_datetime(enhanced_df['Timestamp'])\n",
    "    enhanced_df['Hour'] = enhanced_df['Timestamp'].dt.hour\n",
    "    fraud_by_hour = enhanced_df.groupby(['Hour', 'is_fraud']).size().unstack(fill_value=0)\n",
    "    fraud_by_hour.plot(kind='bar', stacked=True, ax=axes[1, 1], color=['blue', 'red'])\n",
    "    axes[1, 1].set_xlabel('Hour of Day')\n",
    "    axes[1, 1].set_ylabel('Transaction Count')\n",
    "    axes[1, 1].set_title('Transaction Volume by Hour')\n",
    "    axes[1, 1].legend(['Normal', 'Fraud'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nFraud Pattern Statistics:\")\n",
    "    print(f\"Total Fraudulent Transactions: {enhanced_df['is_fraud'].sum():,}\")\n",
    "    print(f\"Actual Fraud Rate: {enhanced_df['is_fraud'].mean()*100:.2f}%\")\n",
    "    print(f\"\\nFraud Type Breakdown:\")\n",
    "    for fraud_type, count in fraud_types.items():\n",
    "        print(f\"  {fraud_type}: {count} ({count/len(fraud_df)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"No fraud patterns found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Merchant Behavior Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze merchant behavior patterns\n",
    "merchant_profiles = enhanced_df.groupby('Merchant').agg({\n",
    "    'Amount': ['mean', 'median', 'std', 'count'],\n",
    "    'TransactionID': 'count'\n",
    "}).round(2)\n",
    "\n",
    "merchant_profiles.columns = ['Avg_Amount', 'Median_Amount', 'Std_Amount', 'Transaction_Count', 'Total_Transactions']\n",
    "merchant_profiles = merchant_profiles.sort_values('Avg_Amount', ascending=False)\n",
    "\n",
    "# Visualize merchant profiles\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Average transaction amount by merchant\n",
    "axes[0, 0].bar(merchant_profiles.index, merchant_profiles['Avg_Amount'], color='steelblue')\n",
    "axes[0, 0].set_xlabel('Merchant')\n",
    "axes[0, 0].set_ylabel('Average Amount ($)')\n",
    "axes[0, 0].set_title('Average Transaction Amount by Merchant')\n",
    "\n",
    "# Transaction volume by merchant\n",
    "axes[0, 1].bar(merchant_profiles.index, merchant_profiles['Transaction_Count'], color='coral')\n",
    "axes[0, 1].set_xlabel('Merchant')\n",
    "axes[0, 1].set_ylabel('Number of Transactions')\n",
    "axes[0, 1].set_title('Transaction Volume by Merchant')\n",
    "\n",
    "# Amount distribution boxplot\n",
    "merchant_amounts = [enhanced_df[enhanced_df['Merchant'] == m]['Amount'].values \n",
    "                    for m in sorted(enhanced_df['Merchant'].unique())]\n",
    "axes[1, 0].boxplot(merchant_amounts, labels=sorted(enhanced_df['Merchant'].unique()))\n",
    "axes[1, 0].set_xlabel('Merchant')\n",
    "axes[1, 0].set_ylabel('Amount ($)')\n",
    "axes[1, 0].set_title('Amount Distribution by Merchant')\n",
    "axes[1, 0].set_ylim(0, 500000)\n",
    "\n",
    "# Merchant type classification\n",
    "merchant_types = {\n",
    "    'A': 'HFT', 'B': 'Block', 'C': 'Arbitrage', 'D': 'Institutional',\n",
    "    'E': 'HFT', 'F': 'Block', 'G': 'Retail Agg', 'H': 'Institutional',\n",
    "    'I': 'Arbitrage', 'J': 'Specialist'\n",
    "}\n",
    "type_counts = pd.Series([merchant_types[m] for m in enhanced_df['Merchant']]).value_counts()\n",
    "axes[1, 1].pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%')\n",
    "axes[1, 1].set_title('Transaction Volume by Merchant Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMerchant Profiles:\")\n",
    "print(merchant_profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Temporal Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add temporal features\n",
    "enhanced_df['Timestamp'] = pd.to_datetime(enhanced_df['Timestamp'])\n",
    "enhanced_df['Hour'] = enhanced_df['Timestamp'].dt.hour\n",
    "enhanced_df['DayOfWeek'] = enhanced_df['Timestamp'].dt.dayofweek\n",
    "enhanced_df['Month'] = enhanced_df['Timestamp'].dt.month\n",
    "\n",
    "# Create temporal heatmap\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Hourly transaction heatmap\n",
    "hourly_pattern = enhanced_df.pivot_table(\n",
    "    values='Amount', \n",
    "    index='Hour', \n",
    "    columns='DayOfWeek', \n",
    "    aggfunc='count', \n",
    "    fill_value=0\n",
    ")\n",
    "sns.heatmap(hourly_pattern, cmap='YlOrRd', annot=False, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Transaction Volume Heatmap (Hour x Day of Week)')\n",
    "axes[0, 0].set_xlabel('Day of Week (0=Monday)')\n",
    "axes[0, 0].set_ylabel('Hour of Day')\n",
    "\n",
    "# Average amount by hour\n",
    "hourly_avg = enhanced_df.groupby('Hour')['Amount'].mean()\n",
    "axes[0, 1].plot(hourly_avg.index, hourly_avg.values, marker='o', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Hour of Day')\n",
    "axes[0, 1].set_ylabel('Average Amount ($)')\n",
    "axes[0, 1].set_title('Average Transaction Amount by Hour')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Fraud concentration by hour (if fraud data exists)\n",
    "if 'is_fraud' in enhanced_df.columns:\n",
    "    fraud_by_hour = enhanced_df.groupby('Hour')['is_fraud'].mean() * 100\n",
    "    axes[1, 0].bar(fraud_by_hour.index, fraud_by_hour.values, \n",
    "                   color=['red' if h in [2, 3, 4, 22, 23] else 'steelblue' \n",
    "                          for h in fraud_by_hour.index])\n",
    "    axes[1, 0].set_xlabel('Hour of Day')\n",
    "    axes[1, 0].set_ylabel('Fraud Rate (%)')\n",
    "    axes[1, 0].set_title('Fraud Rate by Hour (Red = Suspicious Hours)')\n",
    "    axes[1, 0].axhline(y=FRAUD_RATE*100, color='black', linestyle='--', alpha=0.5)\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'No fraud data available', \n",
    "                    ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "\n",
    "# Transaction type distribution over time\n",
    "type_by_hour = enhanced_df.pivot_table(\n",
    "    values='Amount',\n",
    "    index='Hour',\n",
    "    columns='TransactionType',\n",
    "    aggfunc='count',\n",
    "    fill_value=0\n",
    ")\n",
    "type_by_hour.plot(kind='area', stacked=True, ax=axes[1, 1], alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Hour of Day')\n",
    "axes[1, 1].set_ylabel('Transaction Count')\n",
    "axes[1, 1].set_title('Transaction Types by Hour')\n",
    "axes[1, 1].legend(title='Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print temporal statistics\n",
    "print(\"\\nTemporal Pattern Statistics:\")\n",
    "print(f\"Peak Hour: {hourly_avg.idxmax()}:00 (${hourly_avg.max():,.2f} avg)\")\n",
    "print(f\"Quiet Hour: {hourly_avg.idxmin()}:00 (${hourly_avg.min():,.2f} avg)\")\n",
    "if 'is_fraud' in enhanced_df.columns:\n",
    "    print(f\"Highest Fraud Hour: {fraud_by_hour.idxmax()}:00 ({fraud_by_hour.max():.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Statistical Anomaly Detection Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate anomaly scores using simple statistical methods\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Z-score for amounts\n",
    "enhanced_df['amount_zscore'] = np.abs(zscore(enhanced_df['Amount']))\n",
    "\n",
    "# Identify statistical outliers\n",
    "outliers = enhanced_df[enhanced_df['amount_zscore'] > 3]\n",
    "\n",
    "# Visualize outliers\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Scatter plot of amounts with outliers highlighted\n",
    "axes[0].scatter(range(len(enhanced_df)), enhanced_df['Amount'], \n",
    "                alpha=0.1, s=1, label='Normal')\n",
    "axes[0].scatter(outliers.index, outliers['Amount'], \n",
    "                color='red', s=10, label='Statistical Outliers')\n",
    "axes[0].set_xlabel('Transaction Index')\n",
    "axes[0].set_ylabel('Amount ($)')\n",
    "axes[0].set_title('Transaction Amounts with Statistical Outliers')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, enhanced_df['Amount'].quantile(0.99))\n",
    "\n",
    "# Distribution of z-scores\n",
    "axes[1].hist(enhanced_df['amount_zscore'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=3, color='red', linestyle='--', label='Outlier Threshold')\n",
    "axes[1].set_xlabel('Z-Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Amount Z-Scores')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare outliers with actual fraud (if available)\n",
    "print(f\"\\nStatistical Anomaly Detection Results:\")\n",
    "print(f\"Total Statistical Outliers: {len(outliers):,} ({len(outliers)/len(enhanced_df)*100:.2f}%)\")\n",
    "\n",
    "if 'is_fraud' in enhanced_df.columns:\n",
    "    outliers['is_fraud'] = outliers.index.isin(fraud_df['index'])\n",
    "    true_positives = outliers['is_fraud'].sum()\n",
    "    false_positives = len(outliers) - true_positives\n",
    "    total_fraud = enhanced_df['is_fraud'].sum()\n",
    "    false_negatives = total_fraud - true_positives\n",
    "    \n",
    "    precision = true_positives / len(outliers) if len(outliers) > 0 else 0\n",
    "    recall = true_positives / total_fraud if total_fraud > 0 else 0\n",
    "    \n",
    "    print(f\"\\nSimple Statistical Method Performance:\")\n",
    "    print(f\"  True Positives: {true_positives}\")\n",
    "    print(f\"  False Positives: {false_positives}\")\n",
    "    print(f\"  False Negatives: {false_negatives}\")\n",
    "    print(f\"  Precision: {precision:.2%}\")\n",
    "    print(f\"  Recall: {recall:.2%}\")\n",
    "    print(f\"\\nNote: This is a simple baseline. Advanced methods will perform better.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for correlation analysis\n",
    "correlation_df = enhanced_df.copy()\n",
    "correlation_df['Merchant_num'] = correlation_df['Merchant'].map(dict(zip('ABCDEFGHIJ', range(10))))\n",
    "correlation_df['Location_num'] = pd.Categorical(correlation_df['Location']).codes\n",
    "correlation_df['Type_num'] = pd.Categorical(correlation_df['TransactionType']).codes\n",
    "\n",
    "# Select numerical features\n",
    "numerical_features = ['Amount', 'AccountID', 'Merchant_num', 'Location_num', \n",
    "                      'Type_num', 'Hour', 'DayOfWeek']\n",
    "if 'is_fraud' in correlation_df.columns:\n",
    "    correlation_df['is_fraud_num'] = correlation_df['is_fraud'].astype(int)\n",
    "    numerical_features.append('is_fraud_num')\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = correlation_df[numerical_features].corr()\n",
    "\n",
    "# Visualize correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "            cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print strongest correlations\n",
    "if 'is_fraud_num' in numerical_features:\n",
    "    fraud_correlations = correlation_matrix['is_fraud_num'].drop('is_fraud_num').abs().sort_values(ascending=False)\n",
    "    print(\"\\nTop Features Correlated with Fraud:\")\n",
    "    for feature, corr in fraud_correlations.head(5).items():\n",
    "        print(f\"  {feature}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Summary Report for Teachers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary report\n",
    "report = {\n",
    "    'Dataset Overview': {\n",
    "        'Total Transactions': len(enhanced_df),\n",
    "        'Date Range': f\"{enhanced_df['Timestamp'].min()} to {enhanced_df['Timestamp'].max()}\",\n",
    "        'Unique Merchants': enhanced_df['Merchant'].nunique(),\n",
    "        'Unique Accounts': enhanced_df['AccountID'].nunique(),\n",
    "    },\n",
    "    'Amount Statistics': {\n",
    "        'Min': f\"${enhanced_df['Amount'].min():,.2f}\",\n",
    "        'Max': f\"${enhanced_df['Amount'].max():,.2f}\",\n",
    "        'Mean': f\"${enhanced_df['Amount'].mean():,.2f}\",\n",
    "        'Median': f\"${enhanced_df['Amount'].median():,.2f}\",\n",
    "        'Std Dev': f\"${enhanced_df['Amount'].std():,.2f}\"\n",
    "    },\n",
    "    'Benford Analysis': {\n",
    "        'Chi-squared': benford_results['chi_squared'],\n",
    "        'P-value': benford_results['p_value'],\n",
    "        'Compliant': benford_results['compliant']\n",
    "    }\n",
    "}\n",
    "\n",
    "if 'is_fraud' in enhanced_df.columns:\n",
    "    report['Fraud Statistics'] = {\n",
    "        'Total Fraud Cases': enhanced_df['is_fraud'].sum(),\n",
    "        'Fraud Rate': f\"{enhanced_df['is_fraud'].mean()*100:.2f}%\",\n",
    "        'Fraud Types': fraud_df['type'].value_counts().to_dict()\n",
    "    }\n",
    "\n",
    "# Save report\n",
    "import json\n",
    "with open('./data/teacher_summary_report.json', 'w') as f:\n",
    "    json.dump(report, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEACHER SUMMARY REPORT\")\n",
    "print(\"=\"*50)\n",
    "for section, data in report.items():\n",
    "    print(f\"\\n{section}:\")\n",
    "    for key, value in data.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Report saved to: ./data/teacher_summary_report.json\")\n",
    "print(\"\\nFiles Generated:\")\n",
    "print(\"  - enhanced_raw_data.csv (full dataset)\")\n",
    "print(\"  - fraud_patterns_metadata.csv (answer key - DO NOT SHARE)\")\n",
    "print(f\"  - student_dataset_{DIFFICULTY}.csv (unlabeled for students)\")\n",
    "print(f\"  - teacher_answer_key_{DIFFICULTY}.csv (grading reference)\")\n",
    "print(\"  - teacher_summary_report.json (this report)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Grade Student Submissions (Teacher Only)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example: Grade a student submission using grading tools\nfrom grading_tools import evaluate_detection_performance, analyze_pattern_detection, generate_grade_report\n\n# Simulate a student submission (in practice, load actual student CSV)\n# For demo, we'll create a simple prediction\nstudent_predictions = enhanced_df.copy()[['TransactionID']]\nstudent_predictions['is_fraud'] = 0  # Initialize all as normal\n\n# Simple detection: flag high amounts as fraud (naive approach)\nthreshold = enhanced_df['Amount'].quantile(0.98)\nstudent_predictions.loc[enhanced_df['Amount'] > threshold, 'is_fraud'] = 1\n\n# Save simulated submission\nstudent_predictions.to_csv('./data/sample_submission.csv', index=False)\n\n# Grade the submission\nprint(\"Grading submission...\")\nresults = evaluate_detection_performance(\n    './data/sample_submission.csv',\n    './data/fraud_patterns_metadata.csv'\n)\n\n# Generate grade report\nreport = generate_grade_report(results)\nprint(report)\n\n# Analyze pattern detection\nprint(\"\\nPattern-Specific Detection:\")\npattern_analysis = analyze_pattern_detection(\n    './data/sample_submission.csv',\n    './data/fraud_patterns_metadata.csv'\n)\nprint(pattern_analysis)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}